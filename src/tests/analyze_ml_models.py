#!/usr/bin/env python3
"""Script para analisar detalhadamente os modelos de ML e seus resultados."""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, precision_score, recall_score, f1_score
from data_loader import load_seeds_data
from config import FEATURE_NAMES, VARIETY_NAMES, RANDOM_SEED, TEST_SIZE, CV_FOLDS, set_random_seeds
from utils import save_test_output, save_test_results_json, create_directories

def analyze_ml_models():
    """Analisa detalhadamente os modelos de ML com reprodutibilidade garantida."""
    
    # Configurar diret√≥rios de output para testes
    create_directories(is_test=True, subdirs=['ml_models'])
    
    # CR√çTICO: Configurar reprodutibilidade ANTES de qualquer opera√ß√£o
    set_random_seeds()
    
    print("="*80)
    print("AN√ÅLISE DETALHADA DOS MODELOS DE MACHINE LEARNING")
    print("="*80)
    print(f"üîß Configura√ß√£o de reprodutibilidade: RANDOM_SEED = {RANDOM_SEED}")
    
    # Carregar dados
    data = load_seeds_data()
    
    # An√°lise b√°sica usando pandas
    print(f"Dataset shape: {data.shape}")
    print(f"Variedades dispon√≠veis: {list(VARIETY_NAMES.values())}")
    print(f"C√≥digos das variedades: {list(VARIETY_NAMES.keys())}")
    
    # Distribui√ß√£o de classes usando pandas
    class_distribution = data['variety'].value_counts().sort_index()
    print(f"Distribui√ß√£o por variedade:")
    for variety_code, count in class_distribution.items():
        variety_name = VARIETY_NAMES[variety_code]
        percentage = (count / len(data)) * 100
        print(f"  {variety_name} (c√≥digo {variety_code}): {count} amostras ({percentage:.1f}%)")
    
    X = data[FEATURE_NAMES]
    y = data['variety']
    
    # Normaliza√ß√£o
    print("\n1. PREPROCESSAMENTO DOS DADOS")
    print("="*50)
    
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    print("Normaliza√ß√£o Z-score aplicada:")
    
    # Usar pandas para estat√≠sticas mais elegantes
    original_stats = X.describe().loc[['mean', 'std']].round(3)
    
    for i, feature in enumerate(FEATURE_NAMES):
        original_mean = original_stats.loc['mean', feature]
        original_std = original_stats.loc['std', feature]
        scaled_mean = X_scaled[:, i].mean()
        scaled_std = X_scaled[:, i].std()
        print(f"  {feature}:")
        print(f"    Original: Œº={original_mean:.3f}, œÉ={original_std:.3f}")
        print(f"    Normalizado: Œº={scaled_mean:.1e}, œÉ={scaled_std:.3f}")
    
    # Divis√£o treino/teste com configura√ß√£o centralizada
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y, test_size=TEST_SIZE, random_state=RANDOM_SEED, stratify=y
    )
    
    print(f"\nDivis√£o dos dados:")
    print(f"  Total de amostras: {len(data)}")
    print(f"  Conjunto de treino: {len(X_train)} amostras ({len(X_train)/len(data)*100:.1f}%)")
    print(f"  Conjunto de teste: {len(X_test)} amostras ({len(X_test)/len(data)*100:.1f}%)")
    
    print("\nDistribui√ß√£o no conjunto de teste:")
    for variety_code in sorted(y_test.unique()):
        variety_name = VARIETY_NAMES[variety_code]
        count = sum(y_test == variety_code)
        print(f"  {variety_name} (c√≥digo {variety_code}): {count} amostras ({count/len(y_test)*100:.1f}%)")
    
    # Modelos base
    print("\n2. MODELOS BASE (SEM OTIMIZA√á√ÉO)")
    print("="*50)
    
    # Modelos base EXATAMENTE como na documenta√ß√£o (se√ß√£o 4.1)
    base_models = {
        'KNN': KNeighborsClassifier(
            n_neighbors=5,      # Valor padr√£o k=5 para balancear bias-variance
            metric='euclidean', # M√©trica euclidiana inicial
            weights='uniform'   # Peso uniforme para vizinhos
        ),
        'SVM': SVC(
            kernel='rbf',       # Kernel RBF para n√£o-linearidade inicial
            C=1.0,              # Regulariza√ß√£o padr√£o
            gamma='scale',      # Escala autom√°tica do kernel
            random_state=RANDOM_SEED     # Reprodutibilidade
        ),
        'RandomForest': RandomForestClassifier(
            n_estimators=100,   # 100 √°rvores para estabilidade
            max_depth=None,     # Profundidade ilimitada inicialmente
            min_samples_split=2,# Crit√©rio padr√£o de divis√£o
            random_state=RANDOM_SEED     # Reprodutibilidade
        ),
        'LogisticRegression': LogisticRegression(
            penalty='l2',       # Regulariza√ß√£o Ridge
            C=1.0,              # For√ßa de regulariza√ß√£o padr√£o
            solver='lbfgs',     # Otimizador quasi-Newton
            max_iter=1000,      # Itera√ß√µes suficientes para converg√™ncia
            random_state=RANDOM_SEED     # Reprodutibilidade
        ),
        'NaiveBayes': GaussianNB(
            var_smoothing=1e-9  # Suaviza√ß√£o para estabilidade num√©rica
        )
    }
    
    base_results = {}
    
    for name, model in base_models.items():
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        acc = accuracy_score(y_test, y_pred)
        base_results[name] = acc
        print(f"{name}: {acc:.4f} ({int(acc*len(y_test))}/{len(y_test)} corretas)")
    
    # Otimiza√ß√£o de hiperpar√¢metros
    print("\n3. OTIMIZA√á√ÉO DE HIPERPAR√ÇMETROS")
    print("="*50)
    
    # Espa√ßos de busca EXATOS da documenta√ß√£o (se√ß√£o 4.2)
    param_grids = {
        'KNN': {
            'n_neighbors': [3, 5, 7, 9, 11, 15],  # 6 valores: √≠mpar para evitar empates
            'metric': ['euclidean', 'manhattan', 'minkowski'],  # 3 m√©tricas de dist√¢ncia
            'weights': ['uniform', 'distance']  # 2 esquemas de pondera√ß√£o
            # Total: 6 √ó 3 √ó 2 = 36 combina√ß√µes
        },
        'SVM': {
            'C': [0.1, 1, 10, 100],  # 4 valores de regulariza√ß√£o (escala logar√≠tmica)
            'kernel': ['linear', 'rbf', 'poly'],  # 3 tipos de kernel
            'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1]  # 6 valores para kernels n√£o-lineares
            # Total: 4 √ó 3 √ó 6 = 72 combina√ß√µes
        },
        'RandomForest': {
            'n_estimators': [10, 50, 100, 200],  # 4 valores: n√∫mero de √°rvores
            'max_depth': [None, 5, 10, 20],  # 4 valores: profundidade m√°xima
            'min_samples_split': [2, 5, 10],  # 3 valores: amostras m√≠nimas para divis√£o
            'min_samples_leaf': [1, 2, 4]  # 3 valores: amostras m√≠nimas em folhas
            # Total: 4 √ó 4 √ó 3 √ó 3 = 144 combina√ß√µes
        },
        'LogisticRegression': {
            'C': [0.01, 0.1, 1, 10, 100],  # 5 valores de regulariza√ß√£o
            'penalty': ['l2'],  # L2 para compatibilidade
            'solver': ['lbfgs']  # Solver moderno recomendado
            # Total: 5 √ó 1 √ó 1 = 5 combina√ß√µes
        },
        'NaiveBayes': {
            'var_smoothing': [1e-10, 1e-9, 1e-8, 1e-7]  # 4 valores de suaviza√ß√£o
            # Total: 4 combina√ß√µes
        }
    }
    
    optimization_results = {}
    
    for model_name in ['KNN', 'SVM', 'RandomForest', 'LogisticRegression', 'NaiveBayes']:
        print(f"\n{model_name} - Grid Search:")
        
        # Modelo base para otimiza√ß√£o
        if model_name == 'KNN':
            base_model = KNeighborsClassifier()
        elif model_name == 'SVM':
            base_model = SVC(random_state=RANDOM_SEED)
        elif model_name == 'RandomForest':
            base_model = RandomForestClassifier(random_state=RANDOM_SEED)
        elif model_name == 'LogisticRegression':
            base_model = LogisticRegression(random_state=RANDOM_SEED, max_iter=1000)
        elif model_name == 'NaiveBayes':
            base_model = GaussianNB()
        
        param_grid = param_grids[model_name]
        
        # Grid search com CV
        grid_search = GridSearchCV(
            estimator=base_model,
            param_grid=param_grid,
            cv=CV_FOLDS,
            scoring='accuracy',
            n_jobs=1,  # Para garantir reprodutibilidade
            verbose=0
        )
        
        grid_search.fit(X_train, y_train)
        
        optimization_results[model_name] = {
            'best_estimator': grid_search.best_estimator_,
            'best_params': grid_search.best_params_,
            'best_score': grid_search.best_score_,
            'total_combinations': len(grid_search.cv_results_['params'])
        }
        
        print(f"  Total de combina√ß√µes testadas: {optimization_results[model_name]['total_combinations']}")
        print(f"  Melhor configura√ß√£o: {optimization_results[model_name]['best_params']}")
        print(f"  Score CV: {optimization_results[model_name]['best_score']:.4f}")
    
    # Modelos otimizados (usando nomes consistentes com documenta√ß√£o)
    print("\n4. RESULTADOS DOS MODELOS OTIMIZADOS")
    print("="*50)
    
    optimized_models = {}
    for model_name in ['KNN', 'SVM', 'RandomForest', 'LogisticRegression', 'NaiveBayes']:
        if model_name in optimization_results:
            optimized_models[model_name] = optimization_results[model_name]['best_estimator']
        else:
            # Fallback para modelos base se n√£o otimizados
            optimized_models[model_name] = base_models[model_name]
    
    detailed_results = {}
    
    for name, model in optimized_models.items():
        print(f"\n{name}:")
        print("-" * 40)
        
        # Treinar e prever
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        
        # M√©tricas
        acc = accuracy_score(y_test, y_pred)
        precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')
        
        # Cross-validation detalhada
        cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')
        
        print(f"Acur√°cia no teste: {acc:.4f} ({int(acc*len(y_test))}/{len(y_test)} corretas)")
        print(f"Precis√£o (weighted): {precision:.4f}")
        print(f"Recall (weighted): {recall:.4f}")
        print(f"F1-Score (weighted): {f1:.4f}")
        
        print(f"\nCross-validation 5-fold:")
        print(f"  Scores individuais: {[round(s, 4) for s in cv_scores]}")
        print(f"  M√©dia: {cv_scores.mean():.4f}")
        print(f"  Desvio padr√£o: {cv_scores.std():.4f}")
        print(f"  Coeficiente de varia√ß√£o: {(cv_scores.std()/cv_scores.mean())*100:.2f}%")
        
        # Melhoria com otimiza√ß√£o
        if name in base_results:
            improvement = (acc - base_results[name]) / base_results[name] * 100
            print(f"\nMelhoria com otimiza√ß√£o: {improvement:+.2f}%")
        
        # Matriz de confus√£o para os melhores modelos
        if name in ['KNN', 'SVM']:
            cm = confusion_matrix(y_test, y_pred)
            print(f"\nMatriz de confus√£o:")
            # Criar mapeamento usando VARIETY_NAMES
            sorted_variety_codes = sorted(y_test.unique())
            variety_names = [VARIETY_NAMES[code] for code in sorted_variety_codes]
            
            print("  Predito ‚Üí")
            header = "Real ‚Üì    "
            for name in variety_names:
                header += f"{name[:4]:>5}"  # Primeiras 4 letras, alinhadas √† direita
            print(header)
            
            for i, real_name in enumerate(variety_names):
                row = f"  {real_name[:4]:<4}   "  # Nome real √† esquerda
                for j in range(3):
                    row += f"{cm[i,j]:4d} "
                print(row)
            
            # An√°lise de erros usando VARIETY_NAMES do config
            total_errors = len(y_test) - sum(cm[i,i] for i in range(3))
            print(f"\nAn√°lise de erros:")
            print(f"  Total de erros: {total_errors}")
            
            # Criar mapeamento correto usando VARIETY_NAMES
            sorted_variety_codes = sorted(y_test.unique())
            
            for i in range(3):
                for j in range(3):
                    if i != j and cm[i,j] > 0:
                        variety_code_i = sorted_variety_codes[i]
                        variety_code_j = sorted_variety_codes[j]
                        variety_name_i = VARIETY_NAMES[variety_code_i]
                        variety_name_j = VARIETY_NAMES[variety_code_j]
                        print(f"  {variety_name_i} ‚Üí {variety_name_j}: {cm[i,j]} erro(s)")
        
        detailed_results[name] = {
            'accuracy': acc,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'cv_mean': cv_scores.mean(),
            'cv_std': cv_scores.std()
        }
    
    # Feature importance (Random Forest)
    print("\n5. IMPORT√ÇNCIA DAS FEATURES (RANDOM FOREST)")
    print("="*50)
    
    rf_model = optimized_models['RandomForest']
    importances = rf_model.feature_importances_
    indices = np.argsort(importances)[::-1]
    
    print("Ranking de import√¢ncia:")
    cumulative = 0
    for i, idx in enumerate(indices):
        importance = importances[idx]
        cumulative += importance
        print(f"{i+1}. {FEATURE_NAMES[idx]}: {importance:.3f} ({cumulative*100:.1f}% acumulado)")
    
    # Resumo comparativo
    print("\n6. RESUMO COMPARATIVO")
    print("="*50)
    
    print("\nTabela de resultados:")
    print(f"{'Modelo':<20} {'Acur√°cia':<10} {'CV Score':<15} {'Estabilidade'}")
    print("-" * 60)
    
    sorted_models = sorted(detailed_results.items(), key=lambda x: x[1]['accuracy'], reverse=True)
    
    # Criar DataFrame pandas para resultados mais estruturados
    results_data = []
    for name, results in sorted_models:
        cv_str = f"{results['cv_mean']:.4f} ¬± {results['cv_std']:.4f}"
        cv_coef = (results['cv_std'] / results['cv_mean']) * 100
        
        if cv_coef < 5:
            stability = "Excelente"
        elif cv_coef < 10:
            stability = "Boa"
        else:
            stability = "Moderada"
        
        results_data.append({
            'Modelo': name,
            'Acur√°cia': results['accuracy'],
            'CV_Mean': results['cv_mean'],
            'CV_Std': results['cv_std'],
            'Estabilidade': stability
        })
        
        print(f"{name:<20} {results['accuracy']:.4f}     {cv_str:<15} {stability}")
    
    # Salvar resultados em DataFrame para poss√≠vel an√°lise posterior
    results_df = pd.DataFrame(results_data)
    print(f"\nüìä Resultados salvos em DataFrame com {len(results_df)} modelos")
    
    # Preparar dados estruturados para salvamento
    structured_results = {
        'config': {
            'random_seed': RANDOM_SEED,
            'test_size': TEST_SIZE,
            'cv_folds': CV_FOLDS,
            'dataset_shape': data.shape
        },
        'models': detailed_results,
        'summary': {
            'best_model': max(detailed_results.items(), key=lambda x: x[1]['accuracy'])[0],
            'best_accuracy': max(detailed_results.items(), key=lambda x: x[1]['accuracy'])[1]['accuracy'],
            'total_models': len(detailed_results)
        }
    }
    
    # Salvar resultados estruturados em JSON
    save_test_results_json(structured_results, 'ml_models', 'detailed_analysis.json')
    
    # Preparar summary textual
    summary_text = f"""
RESUMO DA AN√ÅLISE ML
{'='*50}

Configura√ß√£o:
- Random Seed: {RANDOM_SEED}
- Test Size: {TEST_SIZE}
- CV Folds: {CV_FOLDS}
- Dataset: {data.shape[0]} amostras, {data.shape[1]} features

Modelos Analisados: {len(detailed_results)}

Melhores Resultados:
"""
    
    for name, results in sorted(detailed_results.items(), key=lambda x: x[1]['accuracy'], reverse=True):
        summary_text += f"\n{name}:"
        summary_text += f"\n  - Acur√°cia: {results['accuracy']:.4f}"
        summary_text += f"\n  - CV Score: {results['cv_mean']:.4f} ¬± {results['cv_std']:.4f}"
        summary_text += f"\n  - Precis√£o: {results['precision']:.4f}"
        summary_text += f"\n  - F1-Score: {results['f1']:.4f}"
    
    # Salvar summary em texto
    save_test_output(summary_text, 'ml_models', 'summary.txt')
    
    print(f"\n‚úÖ Todos os resultados salvos em test_results/ml_models/")
    
    return detailed_results

def generate_documentation_data():
    """Gera dados espec√≠ficos para atualiza√ß√£o da documenta√ß√£o (se√ß√£o 4.2 e 4.3)."""
    
    # Configurar reprodutibilidade
    set_random_seeds()
    
    print("="*80)
    print("GERA√á√ÉO DE DADOS PARA DOCUMENTA√á√ÉO (SE√á√ïES 4.2 e 4.3)")
    print("="*80)
    print(f"üîß RANDOM_SEED = {RANDOM_SEED}, TEST_SIZE = {TEST_SIZE}, CV_FOLDS = {CV_FOLDS}")
    
    # Carregar e preparar dados
    data = load_seeds_data()
    
    print(f"Variedades: {list(VARIETY_NAMES.values())}")
    
    X = data[FEATURE_NAMES]
    y = data['variety']
    
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y, test_size=TEST_SIZE, random_state=RANDOM_SEED, stratify=y
    )
    
    print(f"\nDados: {len(data)} amostras ‚Üí {len(X_train)} treino + {len(X_test)} teste")
    
    # Modelos base EXATAMENTE como na documenta√ß√£o (se√ß√£o 4.1)
    models = {
        'KNN': KNeighborsClassifier(
            n_neighbors=5,      # Valor padr√£o k=5 para balancear bias-variance
            metric='euclidean', # M√©trica euclidiana inicial
            weights='uniform'   # Peso uniforme para vizinhos
        ),
        'SVM': SVC(
            kernel='rbf',       # Kernel RBF para n√£o-linearidade inicial
            C=1.0,              # Regulariza√ß√£o padr√£o
            gamma='scale',      # Escala autom√°tica do kernel
            random_state=RANDOM_SEED     # Reprodutibilidade
        ),
        'RandomForest': RandomForestClassifier(
            n_estimators=100,   # 100 √°rvores para estabilidade
            max_depth=None,     # Profundidade ilimitada inicialmente
            min_samples_split=2,# Crit√©rio padr√£o de divis√£o
            random_state=RANDOM_SEED     # Reprodutibilidade
        ),
        'NaiveBayes': GaussianNB(
            var_smoothing=1e-9  # Suaviza√ß√£o para estabilidade num√©rica
        ),
        'LogisticRegression': LogisticRegression(
            penalty='l2',       # Regulariza√ß√£o Ridge
            C=1.0,              # For√ßa de regulariza√ß√£o padr√£o
            solver='lbfgs',     # Otimizador quasi-Newton
            max_iter=1000,      # Itera√ß√µes suficientes para converg√™ncia
            random_state=RANDOM_SEED     # Reprodutibilidade
        )
    }
    
    # Espa√ßos de busca EXATOS da se√ß√£o 4.2
    param_grids = {
        'KNN': {
            'n_neighbors': [3, 5, 7, 9, 11, 15],
            'metric': ['euclidean', 'manhattan', 'minkowski'],
            'weights': ['uniform', 'distance']
        },
        'SVM': {
            'C': [0.1, 1, 10, 100],
            'kernel': ['linear', 'rbf', 'poly'],
            'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1]
        },
        'RandomForest': {
            'n_estimators': [10, 50, 100, 200],
            'max_depth': [None, 5, 10, 20],
            'min_samples_split': [2, 5, 10],
            'min_samples_leaf': [1, 2, 4]
        },
        'LogisticRegression': {
            'C': [0.01, 0.1, 1, 10, 100],
            'penalty': ['l2'],
            'solver': ['lbfgs']
        },
        'NaiveBayes': {
            'var_smoothing': [1e-10, 1e-9, 1e-8, 1e-7]
        }
    }
    
    print("\nüìä OTIMIZA√á√ÉO DE HIPERPAR√ÇMETROS")
    print("="*60)
    
    optimization_results = {}
    
    for model_name in ['KNN', 'SVM', 'RandomForest', 'LogisticRegression', 'NaiveBayes']:
        model = models[model_name]
        param_grid = param_grids[model_name]
        
        print(f"\nüîç Otimizando {model_name}...")
        
        # Baseline score
        baseline_cv = cross_val_score(model, X_train, y_train, cv=CV_FOLDS, scoring='accuracy')
        baseline_score = baseline_cv.mean()
        
        # Grid search com reprodutibilidade
        grid_search = GridSearchCV(
            estimator=model,
            param_grid=param_grid,
            cv=CV_FOLDS,
            scoring='accuracy',
            n_jobs=1,  # Para garantir reprodutibilidade
            verbose=0
        )
        
        grid_search.fit(X_train, y_train)
        
        optimized_score = grid_search.best_score_
        improvement = optimized_score - baseline_score
        improvement_pct = (improvement / baseline_score) * 100
        
        optimization_results[model_name] = {
            'baseline': baseline_score,
            'optimized': optimized_score,
            'improvement': improvement,
            'improvement_pct': improvement_pct,
            'best_params': grid_search.best_params_,
            'combinations': len(grid_search.cv_results_['params'])
        }
        
        print(f"  ‚úÖ Baseline CV: {baseline_score:.4f} ({baseline_score*100:.2f}%)")
        print(f"  ‚úÖ Otimizado CV: {optimized_score:.4f} ({optimized_score*100:.2f}%)")
        print(f"  ‚úÖ Melhoria: {improvement:+.4f} ({improvement_pct:+.2f}%)")
        print(f"  ‚úÖ Combina√ß√µes: {optimization_results[model_name]['combinations']}")
        print(f"  ‚úÖ Melhor config: {grid_search.best_params_}")
    
    print("\nüìã TABELA PARA SE√á√ÉO 4.2")
    print("="*60)
    print("| Algorithm | Search Space | Best Config | Improvement |")
    print("|-----------|--------------|-------------|-------------|")
    
    for model_name in ['KNN', 'SVM', 'RandomForest', 'LogisticRegression', 'NaiveBayes']:
        result = optimization_results[model_name]
        combinations = result['combinations']
        best_params = result['best_params']
        
        # Formata√ß√£o da configura√ß√£o para a tabela
        if model_name == 'KNN':
            config = f"n_neighbors={best_params['n_neighbors']}, metric='{best_params['metric']}', weights='{best_params['weights']}'"
        elif model_name == 'SVM':
            if 'gamma' in best_params:
                config = f"C={best_params['C']}, kernel='{best_params['kernel']}', gamma='{best_params['gamma']}'"
            else:
                config = f"C={best_params['C']}, kernel='{best_params['kernel']}'"
        elif model_name == 'RandomForest':
            config = f"n_estimators={best_params['n_estimators']}, max_depth={best_params['max_depth']}"
        elif model_name == 'LogisticRegression':
            config = f"C={best_params['C']}, penalty='{best_params['penalty']}', solver='{best_params['solver']}'"
        elif model_name == 'NaiveBayes':
            config = f"var_smoothing={best_params['var_smoothing']}"
        
        improvement = result['improvement_pct']
        if improvement >= 0:
            improvement_str = f'+{improvement:.2f}%'
        else:
            improvement_str = f'{improvement:.2f}%'
            if improvement < -1:
                improvement_str += '*'
        
        print(f"| {model_name} | {combinations} combinations | `{config}` | {improvement_str} |")
    
    print("\nüìä PERFORMANCE NO CONJUNTO DE TESTE")
    print("="*60)
    
    # Modelos otimizados para teste
    test_results = {}
    all_models = ['KNN', 'SVM', 'RandomForest', 'LogisticRegression', 'NaiveBayes']
    
    for model_name in all_models:
        if model_name in optimization_results:
            # Usar par√¢metros otimizados
            best_params = optimization_results[model_name]['best_params']
            if model_name == 'KNN':
                optimized_model = KNeighborsClassifier(**best_params)
            elif model_name == 'SVM':
                optimized_model = SVC(random_state=RANDOM_SEED, **best_params)
            elif model_name == 'RandomForest':
                optimized_model = RandomForestClassifier(random_state=RANDOM_SEED, **best_params)
            elif model_name == 'LogisticRegression':
                optimized_model = LogisticRegression(random_state=RANDOM_SEED, max_iter=1000, **best_params)
            elif model_name == 'NaiveBayes':
                optimized_model = GaussianNB(**best_params)
        else:
            # Usar modelo padr√£o
            optimized_model = models[model_name]
        
        # Treinar e avaliar
        optimized_model.fit(X_train, y_train)
        y_pred = optimized_model.predict(X_test)
        
        # M√©tricas de teste
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, average='weighted')
        recall = recall_score(y_test, y_pred, average='weighted')
        f1 = f1_score(y_test, y_pred, average='weighted')
        
        # Cross-validation no treino
        cv_scores = cross_val_score(optimized_model, X_train, y_train, cv=CV_FOLDS, scoring='accuracy')
        cv_mean = cv_scores.mean()
        cv_std = cv_scores.std()
        
        test_results[model_name] = {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'cv_mean': cv_mean,
            'cv_std': cv_std,
            'correct_predictions': int(round(accuracy * len(y_test))),
            'total_samples': len(y_test)
        }
        
        print(f"\nüéØ {model_name}:")
        print(f"  Acur√°cia teste: {accuracy:.4f} ({test_results[model_name]['correct_predictions']}/{len(y_test)})")
        print(f"  CV Score: {cv_mean:.4f} ¬± {cv_std:.4f}")
    
    print("\nüìã TABELA PARA SE√á√ÉO 4.3")
    print("="*60)
    print("| Algorithm | Accuracy | Precision | Recall | F1-Score | CV Score |")
    print("|-----------|----------|-----------|---------|----------|----------|")
    
    for model_name in all_models:
        if model_name in test_results:
            result = test_results[model_name]
            acc = result['accuracy']
            prec = result['precision']
            rec = result['recall']
            f1 = result['f1']
            cv_mean = result['cv_mean']
            cv_std = result['cv_std']
            
            # Destacar os melhores (threshold ajust√°vel)
            if acc >= 0.888:
                prefix = '**'
                suffix = '**'
            else:
                prefix = ''
                suffix = ''
            
            print(f"| {prefix}{model_name}{suffix} | {prefix}{acc*100:.2f}%{suffix} | {prec*100:.2f}% | {rec*100:.2f}% | {f1*100:.2f}% | {cv_mean*100:.2f}% ¬± {cv_std*100:.2f}% |")
    
    # Identificar melhor modelo
    best_model_name = max(test_results.keys(), key=lambda x: test_results[x]['accuracy'])
    best_result = test_results[best_model_name]
    
    print(f"\nüèÜ MELHOR MODELO IDENTIFICADO")
    print("="*60)
    print(f"Modelo: {best_model_name}")
    print(f"Acur√°cia: {best_result['accuracy']:.4f} = {best_result['accuracy']*100:.2f}%")
    print(f"Predi√ß√µes corretas: {best_result['correct_predictions']}/{best_result['total_samples']}")
    
    if best_model_name in optimization_results:
        print(f"Configura√ß√£o otimizada: {optimization_results[best_model_name]['best_params']}")
    
    print(f"\nüìù DADOS PARA C√ìDIGO DE EXEMPLO NA SE√á√ÉO 4.3")
    print("="*60)
    print(f"total_test_samples = {best_result['total_samples']}  # amostras de teste")
    print(f"correct_predictions = {best_result['correct_predictions']}  # predi√ß√µes corretas")
    print(f"accuracy_manual = {best_result['correct_predictions']}/{best_result['total_samples']} = {best_result['accuracy']:.4f}")
    print(f"# Output: {best_result['accuracy']*100:.2f}%")
    
    return optimization_results, test_results

if __name__ == "__main__":
    # Executar an√°lise completa
    detailed_results = analyze_ml_models()
    
    print("\n" + "="*80)
    print("EXECUTANDO GERA√á√ÉO ESPEC√çFICA PARA DOCUMENTA√á√ÉO")
    print("="*80)
    
    # Executar gera√ß√£o de dados para documenta√ß√£o
    opt_results, test_results = generate_documentation_data()